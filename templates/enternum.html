<!DOCTYPE html>
<html>

<head>
<div id='banner'>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    body {margin:0;}

    .navbar {
      overflow: hidden;
      background-color: Black;
      position: fixed;
      top: 0;
      width: 100%;
    }

    .navbar a {
      float: left;
      display: block;
      color: #f2f2f2;
      text-align: center;
      padding: 30px 16px;
      text-decoration: none;
      font-size: 25px;
    }

    .main {
      padding: 16px;
      margin-top: 30px;
      height: 1500px; /* Used in this example to enable scrolling */
    }
  </style>

<div class="navbar">
  <a href="#home">ARBEN - The Adversarial Robustness Benchmarking tool</a>

  <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
  <meta http-equiv="Pragma" content="no-cache" />
  <meta http-equiv="Expires" content="0" />
  </div>
    </div>

<div id = "everything else">
<center>

<p> <br></p>
<p> <br></p>
<p> <br></p>

<form action="/" method="POST" >
  <p>Select attack:</p>
  <select name="attack">
    <option value="null"> </option>
    <option value="fgsm">FGSM</option>
    <option value="cw">CW</option>
  </select>

  <p>Apply distillation?</p>
  <select name="distillation">
    <option value="null"> </option>
    <option value="distilled">Yes</option>
    <option value="undistilled">No</option>
  </select>
  <br><br>

  <button name="vizbtn" type="submit">Generate MNIST data</button>
</form>

<p></p>

<img src="/static/original.png" width="400" height="333"/>
<img src="/static/adversarial.png" width="400" height="333"/>

</div>

<hr size='2' color="#00000">

<div>
<p  style="margin-left: 40px">
  <b>Understanding adversarial examples through visualisation.</b> 
<p style="margin-left: 40px">
State-of-art classifiers, such as neural networks, can be easily fooled by adversarial examples, i.e. examples that have been carefully computed to be misclassified. Szegedy stumbled upon this finding by accident in 2014, when he tried to understand the underlying process causing deep learning models to make their classification decisions. His experiment consisted of making a visualization that gradually transformed one image to another, in the hopes of identifying which features of the target image would start to appear on the original one. The discovery he made was remarkable: the modification required when the CNN changed its prediction, the resulting modifications were imperceptible to human eye.   
<p style="margin-left: 40px">
State-of-art classifiers, such as deepneural networks, can be easily fooled. By making small input modifications, these machine learning algorithms start to behave very differby making small modifications to their inputs. This phenomenon is called adversarial machine learning. In the image domain, small pixel perturbations misclassify otherwise correctly predicted images. These perturbations can be so smallthe result
</p>
</div>

</center>
</body>
</html>

